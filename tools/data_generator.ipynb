{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    @author  jasonYu\n",
    "    @date    2017/6/3\n",
    "    @version created\n",
    "    @email   yuquanjie13@gmail.com\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "sys.path.append('/home/yuquanjie/Documents/deep-direct-regression/tools')\n",
    "from point_check import point_in_polygon\n",
    "\n",
    "'''\n",
    "1. generate intput data\n",
    "    1.1 input image resizing, from (2400,3200) to (320,320)\n",
    "    1.2 (TODO) zero-center by mean pixel??\n",
    "    1.3 proprecess image\n",
    "2. generate output data\n",
    "    2.1 generate classification output data\n",
    "    2.2 (TODO) generate regression output data\n",
    "'''\n",
    "def get_train_data(all_imgs):\n",
    "    visulise = False\n",
    "    while True:\n",
    "        for img_data in all_imgs:      \n",
    "            print img_data['imagePath']\n",
    "            # image file wheater corresponding to text fle\n",
    "            annot = img_data['imagePath']\n",
    "            strinfo = re.compile('image/')\n",
    "            annot = strinfo.sub('text/',annot)\n",
    "            strinfo = re.compile('jpg')\n",
    "            annot = strinfo.sub('txt',annot)\n",
    "            \n",
    "            if os.path.isfile(img_data['imagePath']) and os.path.isfile(annot):\n",
    "                img = cv2.imread(img_data['imagePath'])\n",
    "                width = img.shape[0] #2400\n",
    "                height = img.shape[1] #3200\n",
    "\n",
    "                ## 1)generate input data\n",
    "                ### 1.1)input image, from (2400,3200) to (320,320)\n",
    "                img_320 = cv2.resize(img,(320,320),interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                ## 2)generate output data\n",
    "                ### 2.1)generate classification output data\n",
    "                divi_x = float(height) / 80.0\n",
    "                divi_y = float(width) / 80.0\n",
    "                #y_class_lable = -1 * np.ones((80,80)).astype(np.float32)\n",
    "                y_class_lable = -1 * np.ones((80,80))\n",
    "                for ix in xrange(y_class_lable.shape[0]):\n",
    "                    for jy in xrange(y_class_lable.shape[1]):\n",
    "                        for polygon in img_data['boxCoord']:\n",
    "                            x1 = string.atof(polygon[0]) / divi_x\n",
    "                            x2 = string.atof(polygon[2]) / divi_x\n",
    "                            x3 = string.atof(polygon[4]) / divi_x\n",
    "                            x4 = string.atof(polygon[6]) / divi_x\n",
    "\n",
    "                            y1 = string.atof(polygon[1]) / divi_y\n",
    "                            y2 = string.atof(polygon[3]) / divi_y\n",
    "                            y3 = string.atof(polygon[5]) / divi_y\n",
    "                            y4 = string.atof(polygon[7]) / divi_y\n",
    "\n",
    "                            polygon = [(x1,y1), (x2,y2), (x3,y3), (x4,y4)]\n",
    "                            if point_in_polygon(ix,jy,polygon):\n",
    "                                y_class_lable[ix][jy] = 1\n",
    "                            #else:\n",
    "                            #    y_class_lable[ix][jy] = 0   \n",
    "\n",
    "                if visulise:\n",
    "                    if img_data['imagePath'] == '/home/yuquanjie/Documents/icdar2017rctw_train_v1.2/train/part1/image/image_100.jpg' :\n",
    "                        img = cv2.imread(img_data['imagePath'])\n",
    "                        img_80 = cv2.resize(img,(80,80),interpolation=cv2.INTER_CUBIC)\n",
    "                        img_draw = Image.fromarray(cv2.cvtColor(img_80,cv2.COLOR_BGR2RGB))\n",
    "                        draw = ImageDraw.Draw(img_draw)\n",
    "                        for coord in img_data['boxCoord']:\n",
    "                            print 'detail'\n",
    "                            print float(coord[0]) / divi_x, float(coord[1]) / divi_y\n",
    "                            print float(coord[2]) / divi_x, float(coord[3]) / divi_y\n",
    "                            print float(coord[4]) / divi_x, float(coord[5]) / divi_y\n",
    "                            print float(coord[6]) / divi_x, float(coord[7]) / divi_y\n",
    "                            print 'detail'\n",
    "                            draw.polygon([(float(coord[0]) / divi_x, float(coord[1]) / divi_y), \n",
    "                                          (float(coord[2]) / divi_x, float(coord[3]) / divi_y),\n",
    "                                          (float(coord[4]) / divi_x, float(coord[5]) / divi_y), \n",
    "                                          (float(coord[6]) / divi_x, float(coord[7]) / divi_y)],\n",
    "                                         outline = \"red\",fill=\"blue\")\n",
    "                        img_draw = np.array(img_draw)\n",
    "                        img_draw = cv2.cvtColor(img_draw, cv2.COLOR_RGB2BGR)\n",
    "                        one_locs = np.where(y_class_lable > 0)\n",
    "                        print one_locs\n",
    "                        print len(one_locs[0])\n",
    "                        print img_data['imagePath']\n",
    "                        print img_data['boxNum']\n",
    "                        cv2.imshow('img',img_draw)\n",
    "                        cv2.waitKey(0)        \n",
    "                \n",
    "                img_320 = np.expand_dims(img_320,axis = 0)\n",
    "                one_locs = np.where(y_class_lable > 0)\n",
    "                y_class_lable = np.expand_dims(y_class_lable,axis = 0)\n",
    "                y_class_lable = np.expand_dims(y_class_lable,axis = 3)\n",
    "                #yield np.copy(img_320), np.copy(y_class_lable), img_data\n",
    "                \n",
    "                ### 2.2)(TODO) generate regression output data\n",
    "                #y_regr_lable = np.zeros((80,80,8)).astype(np.float32)                   \n",
    "                y_regr_lable = np.zeros((80,80,8))\n",
    "                for i in xrange(len(one_locs[0])):\n",
    "                    # get quadrilateral vertex 4 corrdinates\n",
    "                    for polygon in img_data['boxCoord']:\n",
    "                        x1 = string.atof(polygon[0]) / divi_x\n",
    "                        x2 = string.atof(polygon[2]) / divi_x\n",
    "                        x3 = string.atof(polygon[4]) / divi_x\n",
    "                        x4 = string.atof(polygon[6]) / divi_x\n",
    "\n",
    "                        y1 = string.atof(polygon[1]) / divi_y\n",
    "                        y2 = string.atof(polygon[3]) / divi_y\n",
    "                        y3 = string.atof(polygon[5]) / divi_y\n",
    "                        y4 = string.atof(polygon[7]) / divi_y\n",
    "\n",
    "                        poly = [(x1,y1), (x2,y2), (x3,y3), (x4,y4)]\n",
    "                        ix = one_locs[0][i]\n",
    "                        jy = one_locs[1][i]\n",
    "                        if point_in_polygon(ix,jy,poly):\n",
    "                            left_top_x = poly[0][0]\n",
    "                            left_top_y = poly[0][1]\n",
    "                            righ_top_x = poly[1][0]\n",
    "                            righ_top_y = poly[1][1]\n",
    "\n",
    "                            righ_dow_x = poly[2][0]\n",
    "                            righ_dow_y = poly[2][1] \n",
    "                            left_dow_x = poly[3][0]\n",
    "                            left_dow_y = poly[3][1]\n",
    "\n",
    "                            y_regr_lable[ix][jy][0] = left_top_x * 4 - ix * 4\n",
    "                            y_regr_lable[ix][jy][1] = left_top_y * 4 - jy * 4\n",
    "                            y_regr_lable[ix][jy][2] = righ_top_x * 4 - ix * 4\n",
    "                            y_regr_lable[ix][jy][3] = righ_top_y * 4 - jy * 4\n",
    "\n",
    "                            y_regr_lable[ix][jy][4] = righ_dow_x * 4 - ix * 4\n",
    "                            y_regr_lable[ix][jy][5] = righ_dow_y * 4 - jy * 4\n",
    "                            y_regr_lable[ix][jy][6] = left_dow_x * 4 - ix * 4\n",
    "                            y_regr_lable[ix][jy][7] = left_dow_y * 4 - jy * 4\n",
    "                if visulise and img_data['imagePath'] == '/home/yuquanjie/Documents/icdar2017rctw_train_v1.2/train/part1/image/image_100.jpg' :\n",
    "                    print y_regr_lable[59][75]\n",
    "                    print y_regr_lable[59][76]\n",
    "                    print y_regr_lable[59][77]\n",
    "                y_regr_lable = np.expand_dims(y_regr_lable,axis = 0)\n",
    "                yield np.copy(img_320), np.copy(y_class_lable), np.copy(y_regr_lable), img_data\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "HUBER_DELTA = 1.0\n",
    "\n",
    "\n",
    "def smoothL1(y_true, y_pred):\n",
    "    #print y_pred.shape\n",
    "    import tensorflow as tf\n",
    "    #1. slice\n",
    "    #conTmp = tf.slice(y_true, [0, 0, 0, 8],[1, 80, 80, 1])\n",
    "    #2. concatenate\n",
    "    #tmp = tf.expand_dims(y_true[:, :, :, 8], 3)  page 27 helped by hl\n",
    "    tmp = tf.expand_dims(y_true[:, :, :, 8], 3)\n",
    "    #print tmp\n",
    "    \n",
    "    \n",
    "    y_true = tf.concat([y_true, tmp], 3)\n",
    "    y_true = tf.concat([y_true, tmp], 3)\n",
    "    y_true = tf.concat([y_true, tmp], 3)\n",
    "    \n",
    "    y_true = tf.concat([y_true, tmp], 3)\n",
    "    y_true = tf.concat([y_true, tmp], 3)\n",
    "    y_true = tf.concat([y_true, tmp], 3)\n",
    "    y_true = tf.concat([y_true, tmp], 3)\n",
    "    \n",
    "    #print y_true\n",
    "    x = K.abs(y_true[:, :, :, 0:8] - y_pred)\n",
    "    if K._BACKEND == 'tensorflow':\n",
    "        import tensorflow as tf\n",
    "        x = tf.where(tf.greater(HUBER_DELTA, x), \n",
    "                     0.5 * x ** 2, \n",
    "                     x - 0.5)\n",
    "        x = tf.where(tf.greater(y_true[:, :, :,8:16], 0),\n",
    "                     y_true[:, :, :,8:16],\n",
    "                     0 * y_true[:, :, :,8:16]) * x\n",
    "        #return  K.sum(x)\n",
    "        return  K.mean(x, axis = -1)\n",
    "#def mergeLoss():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Add, Dense, Activation, Flatten, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.convolutional import Conv2DTranspose\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "def multi_task(input_tensor=None, trainable=False):\n",
    "    img_input = input_tensor\n",
    "    bn_axis = 3\n",
    "    \n",
    "    #conv_1\n",
    "    conv1_1 = Convolution2D(32, (5, 5), strides=(1,1), padding='same',\n",
    "                            activation='relu', name='conv1_1')(img_input)\n",
    "    pool1 = MaxPooling2D((2,2), strides=(2,2), name='pool1')(conv1_1)\n",
    "           \n",
    "    #conv_2\n",
    "    conv2_1 = Convolution2D(64, (3, 3), strides=(1,1), padding='same',\n",
    "                            activation='relu', name='conv2_1')(pool1)\n",
    "    conv2_2 = Convolution2D(64, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv2_2')(conv2_1)\n",
    "    pool2 = MaxPooling2D((2,2), strides=(2,2), name='pool2')(conv2_2)\n",
    "    \n",
    "    #conv_3    \n",
    "    conv3_1 = Convolution2D(128, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv3_1')(pool2)\n",
    "    conv3_2 = Convolution2D(128, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv3_2')(conv3_1)\n",
    "    pool3 = MaxPooling2D((2,2), strides=(2,2), name='pool3')(conv3_2)\n",
    "    pool3_for_fuse = Convolution2D(128, (1, 1), strides=(1,1), padding='same',\n",
    "                                   activation='relu', name='pool3_for_fuse')(pool3)\n",
    "    \n",
    "    #conv_4    \n",
    "    conv4_1 = Convolution2D(256, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv4_1')(pool3)\n",
    "    conv4_2 = Convolution2D(256, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv4_2')(conv4_1)\n",
    "    pool4 = MaxPooling2D((2,2), strides=(2,2), name='pool4')(conv4_2)\n",
    "    pool4_for_fuse = Convolution2D(128, (1, 1), strides=(1,1), padding='same',\n",
    "                                   activation='relu', name='pool4_for_fuse')(pool4)\n",
    "    \n",
    "    #conv_5    \n",
    "    conv5_1 = Convolution2D(512, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv5_1')(pool4)\n",
    "    conv5_2 = Convolution2D(512, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv5_2')(conv5_1)\n",
    "    pool5 = MaxPooling2D((2,2), strides=(2,2), name='pool5')(conv5_2)\n",
    "    pool5_for_fuse = Convolution2D(128, (1, 1), strides=(1,1), padding='same',\n",
    "                                   activation='relu', name='pool5_for_fuse')(pool5)\n",
    "    \n",
    "    #conv_6    \n",
    "    conv6_1 = Convolution2D(512, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv6_1')(pool5)\n",
    "    conv6_2 = Convolution2D(512, (3, 3), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv6_2')(conv6_1)\n",
    "    pool6 = MaxPooling2D((2,2), strides=(2,2), name='pool6')(conv6_2)\n",
    "    \n",
    "    #\n",
    "    conv7_1 = Convolution2D(128, (1, 1), strides=(1,1), padding='same', \n",
    "                            activation='relu', name='conv7_1')(pool6)\n",
    "    \n",
    "    upscore2 = Conv2DTranspose(filters=128, kernel_size=(2, 2),\n",
    "                              strides=(2, 2), padding='valid', use_bias=False,\n",
    "                             name='upscore2')(conv7_1)\n",
    "    \n",
    "    fuse_pool5 = add([upscore2, pool5_for_fuse])\n",
    "    upscore4 = Conv2DTranspose(filters=128, kernel_size=(2, 2),\n",
    "                              strides=(2, 2), padding='valid', use_bias=False,\n",
    "                             name='upscore4')(fuse_pool5)\n",
    "    fuse_pool4 = add([upscore4, pool4_for_fuse])\n",
    "       \n",
    "    upscore8 = Conv2DTranspose(filters=128, kernel_size=(2, 2),\n",
    "                              strides=(2, 2), padding='valid', use_bias=False,\n",
    "                             name='upscore8')(fuse_pool4)\n",
    "    fuse_pool3 = add([upscore8, pool3_for_fuse])\n",
    "    \n",
    "    upscore16 = Conv2DTranspose(filters=128, kernel_size=(2, 2),\n",
    "                              strides=(2, 2), padding='valid', use_bias=False,\n",
    "                             name='upscore16')(fuse_pool3)\n",
    "    ##########################################################################\n",
    "    ####### shared layer\n",
    "    ##########################################################################\n",
    "    x_clas = Convolution2D(1, (1, 1), strides=(1,1), padding='same', name='out_class')(upscore16)\n",
    "    x = Convolution2D(128, (1, 1), strides=(1,1), padding='same', activation='relu')(upscore16)\n",
    "    x = Convolution2D(8, (1, 1), strides=(1,1), padding='same', activation='sigmoid')(x)\n",
    "    x_regr = Lambda(lambda t: 800 * t - 400)(x)\n",
    "    #x_merge = Merge(x_)\n",
    "    return [x_clas, x_regr, x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n",
      "(101, 320, 320, 3)\n",
      "(101, 80, 80, 1)\n",
      "(101, 80, 80, 9)\n",
      "Epoch 1/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 38.7326Epoch 00000: loss improved from inf to 38.56970, saving model to /home/yuquanjie/Documents/deep-direct-regression/model/loss-decrease-00-38.57.hdf5\n",
      "101/101 [==============================] - 13s - loss: 38.5697    \n",
      "Epoch 2/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.1531Epoch 00001: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1490     \n",
      "Epoch 3/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.4287Epoch 00002: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1810     \n",
      "Epoch 4/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.5342Epoch 00003: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 5/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.4338Epoch 00004: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 6/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 38.8148Epoch 00005: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 7/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.1940Epoch 00006: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 8/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 38.9147Epoch 00007: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 9/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.1444Epoch 00008: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 10/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.2385Epoch 00009: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 11/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.0283Epoch 00010: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 12/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.4561Epoch 00011: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 13/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 38.8719Epoch 00012: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 14/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.4075Epoch 00013: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 15/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.5423Epoch 00014: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 16/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.5192Epoch 00015: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 17/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.2859Epoch 00016: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 18/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.3294Epoch 00017: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 19/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.3889Epoch 00018: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 20/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.5464Epoch 00019: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 21/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.0210Epoch 00020: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 22/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.1946Epoch 00021: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 23/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.4985Epoch 00022: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 24/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.0006Epoch 00023: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 25/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.3608Epoch 00024: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 26/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.2210Epoch 00025: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 27/500\n",
      "100/101 [============================>.] - ETA: 0s - loss: 39.2859Epoch 00026: loss did not improve\n",
      "101/101 [==============================] - 4s - loss: 39.1853     \n",
      "Epoch 28/500\n",
      " 95/101 [===========================>..] - ETA: 0s - loss: 40.8029"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-a61151286f16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0mloss_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultask_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;31m#loss_class = multask_model.fit(X, Y, batch_size=5, epochs=500, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from keras.layers import Input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py\n",
    "from keras.models import Model\n",
    "import sys\n",
    "sys.path.append('/home/yuquanjie/Documents/deep-direct-regression/tools')\n",
    "from point_check import point_in_polygon\n",
    "from get_data import get_raw_data\n",
    "import os\n",
    "gpu_id = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(gpu_id)\n",
    "\n",
    "all_imgs, numFileTxt = get_raw_data('/home/yuquanjie/Documents/icdar2017rctw_train_v1.2/train/part1')\n",
    "data_gen_train = get_train_data(all_imgs)\n",
    "\n",
    "img_input = Input((320,320,3))\n",
    "# Create model\n",
    "multi = multi_task(img_input,trainable=True)\n",
    "#multask_model = Model(img_input, multi[0:2])\n",
    "multask_model = Model(img_input, multi[1])\n",
    "#print multask_model.summary()\n",
    "sgd = optimizers.SGD(lr=0.01, decay=4e-4, momentum=0.9)\n",
    "# Compile model\n",
    "#multask_model.compile(loss=['hinge',smoothL1], optimizer=sgd)\n",
    "multask_model.compile(loss=[smoothL1], optimizer=sgd)\n",
    "\n",
    "if False:\n",
    "    X_train, Y_train_cls, Y_train_regr, Z  = data_gen_train.next()\n",
    "    Y_train_merge = np.concatenate([Y_train_regr, Y_train_cls], axis = 3)\n",
    "    for i in range (100):\n",
    "        X_train_iter, Y_train_cls_iter, Y_train_regr_iter, Z  = data_gen_train.next()\n",
    "\n",
    "        X_train = np.concatenate([X_train, X_train_iter], axis = 0)\n",
    "        Y_train_cls = np.concatenate([Y_train_cls, Y_train_cls_iter], axis = 0)\n",
    "\n",
    "        Y_train_merge_iter = np.concatenate([Y_train_regr_iter, Y_train_cls_iter], axis = 3)\n",
    "        Y_train_merge = np.concatenate([Y_train_merge, Y_train_merge_iter], axis = 0)\n",
    "\n",
    "\n",
    "X = X_train\n",
    "Y = [Y_train_cls, Y_train_merge]\n",
    "Y_1 = Y[0]\n",
    "Y_2 = Y[1]\n",
    "\n",
    "read_data = False\n",
    "if read_data:\n",
    "    # wirte data\n",
    "    file = h5py.File('trian_dataset_test.h5','w')\n",
    "    file.create_dataset('X_train',data = X_train)\n",
    "    file.create_dataset('Y_train_cls',data = Y_train_cls)\n",
    "    file.create_dataset('Y_train_merge',data = Y_train_merge)\n",
    "    file.close()   \n",
    "    # read data\n",
    "    print \"reading data from trian_dataset_test.h5 \"\n",
    "    file = h5py.File('trian_dataset_test.h5','r')\n",
    "    X = file['X_train'][:]\n",
    "    Y_1 = file['Y_train_cls'][:]\n",
    "    Y_2 = file['Y_train_merge'][:]\n",
    "    file.close()\n",
    "\n",
    "print X.shape\n",
    "print Y_1.shape\n",
    "print Y_2.shape\n",
    "Y = [Y_1, Y_2]\n",
    "# checkpoint \n",
    "filepath = \"/home/yuquanjie/Documents/deep-direct-regression/model/loss-decrease-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, \n",
    "                             save_best_only = True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "loss_class = multask_model.fit(X, Y_2, batch_size=5, epochs=500,callbacks = callbacks_list, verbose=1)\n",
    "#loss_class = multask_model.fit(X, Y, batch_size=5, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from keras.layers import Input\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py\n",
    "from keras.models import Model, load_model\n",
    "import sys\n",
    "sys.path.append('/home/yuquanjie/Documents/deep-direct-regression/tools')\n",
    "from point_check import point_in_polygon\n",
    "from get_data import get_raw_data\n",
    "import os\n",
    "gpu_id = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(gpu_id)\n",
    "\n",
    "\n",
    "all_imgs, numFileTxt = get_raw_data('/home/yuquanjie/Documents/icdar2017rctw_train_v1.2/train/part1')\n",
    "data_gen_train = get_train_data(all_imgs)\n",
    "#######\n",
    "if True:\n",
    "    X_train, Y_train_cls, Y_train_regr, Z  = data_gen_train.next()\n",
    "    Y_train_merge = np.concatenate([Y_train_regr, Y_train_cls], axis = 3)\n",
    "    for i in range (0):\n",
    "        X_train_iter, Y_train_cls_iter, Y_train_regr_iter, Z  = data_gen_train.next()\n",
    "\n",
    "        X_train = np.concatenate([X_train, X_train_iter], axis = 0)\n",
    "        Y_train_cls = np.concatenate([Y_train_cls, Y_train_cls_iter], axis = 0)\n",
    "\n",
    "        Y_train_merge_iter = np.concatenate([Y_train_regr_iter, Y_train_cls_iter], axis = 3)\n",
    "        Y_train_merge = np.concatenate([Y_train_merge, Y_train_merge_iter], axis = 0)\n",
    "\n",
    "\n",
    "X = X_train\n",
    "print X.shape\n",
    "Y = [Y_train_cls, Y_train_merge]\n",
    "#######\n",
    "\n",
    "\n",
    "img_input = Input((320,320,3))\n",
    "# Create model\n",
    "multi = multi_task(img_input,trainable=True)\n",
    "multask_model = Model(img_input, multi[0:2])\n",
    "#print multask_model.summary()\n",
    "sgd = optimizers.SGD(lr=0.01, decay=4e-4, momentum=0.9)\n",
    "# Compile model\n",
    "multask_model.compile(loss=['hinge',smoothL1], optimizer=sgd)\n",
    "\n",
    "final_model = load_model('model/loss-decrease-999-4.07.hdf5', custom_objects={'smoothL1': smoothL1})\n",
    "P_result = final_model.predict_on_batch(X)\n",
    "print P_result[0].shape\n",
    "print P_result[1].shape\n",
    "print np.where(P_result[0][0, :, :, 0] > 0)\n",
    "np.set_printoptions(threshold=1e6)\n",
    "print P_result[1][0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: libcusolver.so.7.5: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7815ef53b8f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m a = tf.constant([[[1, 1, 1, 1], \n\u001b[0;32m      3\u001b[0m                  \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  [0, 0, 0, 0]],\n\u001b[0;32m      5\u001b[0m                 [[0, 0, 0, 0], \n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: libcusolver.so.7.5: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([[[1, 1, 1, 1], \n",
    "                 [1, 1, 1, 1], \n",
    "                 [0, 0, 0, 0]],\n",
    "                [[0, 0, 0, 0], \n",
    "                 [0, 0, 0, 0], \n",
    "                 [0, 0, 0, 0]]\n",
    "                ])\n",
    "#num_pos = tf.reduce_sum(a)\n",
    "#print num_pos\n",
    "#with tf.Session() as sess:\n",
    "#    print(sess.run(a))\n",
    "#    print(sess.run(los))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}